{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8aefa3c",
   "metadata": {},
   "source": [
    "### CODE TO COMBINE ALL CASE SUMMARIES TO MAKE PRE TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c88979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your folder path containing the case text files\n",
    "folder_path = 'G:\\\\college stuff\\\\sem6\\\\NLP project indian\\\\pre-train-data\\\\dataset'  # Replace with your actual path\n",
    "output_file = 'combined_cases.txt'\n",
    "\n",
    "# Create/open the output file in write mode\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                case_text = infile.read().strip()\n",
    "                outfile.write(\"<START CASE>\\n\")\n",
    "                outfile.write(case_text + \"\\n\")\n",
    "                outfile.write(\"<END CASE>\\n\\n\")\n",
    "\n",
    "print(f\"All cases have been combined into '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f8c2f",
   "metadata": {},
   "source": [
    "### CODE TO SUMMARIES CASE TEXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a19270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "from transformers import BartTokenizer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import ollama\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"llama3.1:8b\"\n",
    "\n",
    "model = Ollama(model = MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" \n",
    "You are a legal assistant AI trained to analyze Indian legal cases. Your task is to extract and present key information from the given case in a structured and organized format.\n",
    "\n",
    "Analyze the case and return the following sections with appropriate headings:\n",
    "\n",
    "- FACTS\n",
    "- ARGUMENTS\n",
    "- OBSERVATIONS\n",
    "- JUDGMENT\n",
    "\n",
    "If any section is missing or unclear, state: \"Information not available\" under that heading.\n",
    "\n",
    "Format your response using XML-like tags for each section like this:\n",
    "\n",
    "<FACTS>\n",
    "[Extracted facts]\n",
    "</FACTS>\n",
    "\n",
    "<ARGUMENTS>\n",
    "[Extracted arguments]\n",
    "</ARGUMENTS>\n",
    "\n",
    "<OBSERVATIONS>\n",
    "[Extracted observations]\n",
    "</OBSERVATIONS>\n",
    "\n",
    "<JUDGMENT>\n",
    "[Extracted judgment]\n",
    "</JUDGMENT>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69962224",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"G:\\\\college stuff\\\\sem6\\\\NLP project indian\\\\pre-train-data\\\\new\"\n",
    "output_folder = \"G:\\\\college stuff\\\\sem6\\\\NLP project indian\\\\pre-train-data\\\\dataset\"\n",
    "move_folder = \"G:\\\\college stuff\\\\sem6\\\\NLP project indian\\\\pre-train-data\\\\processed\"\n",
    "\n",
    "# Get total number of files for the progress bar\n",
    "total_files = 0\n",
    "for _, _, files in os.walk(folder_path):\n",
    "    total_files += len(files)\n",
    "\n",
    "# Create a progress bar\n",
    "progress_bar = tqdm(total=total_files, desc=\"Processing Files\", unit=\"file\")\n",
    "\n",
    "# List all files in the folder\n",
    "file_count = 0\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)  # Create full file path\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            case_text = f.read()\n",
    "        \n",
    "        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "        tokens = tokenizer(case_text, return_tensors=\"pt\")['input_ids']\n",
    "        \n",
    "        # Create a token splitter based on BART's token size limit\n",
    "        token_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=100)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = token_splitter.split_text(case_text)\n",
    "        \n",
    "        input_text = f\"{case_text}\\n\\nQuestion: {prompt}\\nAnswer:\"\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model='llama3.1:8b',\n",
    "            messages=[{'role': 'user', 'content': input_text}]\n",
    "        )\n",
    "        \n",
    "        # Write the output to the summary file\n",
    "        output_file_path = os.path.join(output_folder, filename.replace(\".txt\", \"_pretrain.txt\"))\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            out_file.write(response['message']['content'])\n",
    "        \n",
    "        # Move the processed file to the move_folder\n",
    "        destination_path = os.path.join(move_folder, filename)\n",
    "        shutil.move(file_path, destination_path)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"Current File\": filename})\n",
    "        \n",
    "        file_count += 1\n",
    "\n",
    "# Close the progress bar when done\n",
    "progress_bar.close()\n",
    "print(f\"Completed processing {file_count} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf973a5",
   "metadata": {},
   "source": [
    "### Creating Pre-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c44dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your folder path containing the case text files\n",
    "folder_path = 'G:\\\\college stuff\\\\sem6\\\\NLP project indian\\\\pre-train-data\\\\dataset'  # Replace with your actual path\n",
    "output_file = 'combined_cases.txt'\n",
    "\n",
    "# Create/open the output file in write mode\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                case_text = infile.read().strip()\n",
    "                outfile.write(\"<START CASE>\\n\")\n",
    "                outfile.write(case_text + \"\\n\")\n",
    "                outfile.write(\"<END CASE>\\n\\n\")\n",
    "\n",
    "print(f\"All cases have been combined into '{output_file}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
